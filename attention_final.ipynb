{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "reference:https://keras.io/examples/nlp/lstm_seq2seq/\n",
        "\n",
        "https://arxiv.org/pdf/1409.0473.pdf"
      ],
      "metadata": {
        "id": "N52u5rgQw-Hd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "reference: https://machinelearningmastery.com/define-encoder-decoder-sequence-sequence-model-neural-machine-translation-keras/"
      ],
      "metadata": {
        "id": "G1U2YR4jwaPT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6VgLpCUITeG0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb\n",
        "import wandb\n",
        "from wandb.keras import WandbCallback"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aqLE759CP8QC",
        "outputId": "454a1da8-c92c-40ef-c2fd-f6c1e882a711"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.7/dist-packages (0.12.16)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.5.12)\n",
            "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.0.9)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from wandb) (57.4.0)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.7/dist-packages (from wandb) (0.1.2)\n",
            "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.1.27)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.7/dist-packages (from wandb) (1.2.3)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.2.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.0.9)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (5.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8RMV62EDbr_l"
      },
      "outputs": [],
      "source": [
        "def load_data(path):\n",
        "    with open(path) as fil:\n",
        "        data = pd.read_csv(fil,sep='\\t',header=None,names=[\"hi\",\"en\",\"\"],skip_blank_lines=True,index_col=None)\n",
        "    data = data[['hi','en']]\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the dataset\n",
        "!curl https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar --output daksh.tar\n",
        "# Extract the downloaded tar file\n",
        "!tar -xvf  'daksh.tar'\n",
        "# Set the file paths to train, validation and test dataset\n",
        "train_file=os.path.join(os.getcwd(),\"dakshina_dataset_v1.0\",\"hi\",\"lexicons\",\"hi.translit.sampled.train.tsv\")\n",
        "vaildation_file_path = os.path.join(os.getcwd(),\"dakshina_dataset_v1.0\",\"hi\",\"lexicons\",\"hi.translit.sampled.dev.tsv\")\n",
        "test_file_path = os.path.join(os.getcwd(),\"dakshina_dataset_v1.0\",\"hi\",\"lexicons\",\"hi.translit.sampled.test.tsv\")"
      ],
      "metadata": {
        "id": "jQd76Coie5TM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fa8826b-c7ad-44a4-ed48-384fd5692bba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 1915M  100 1915M    0     0   227M      0  0:00:08  0:00:08 --:--:--  252M\n",
            "dakshina_dataset_v1.0/bn/\n",
            "dakshina_dataset_v1.0/bn/lexicons/\n",
            "dakshina_dataset_v1.0/bn/lexicons/bn.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/bn/lexicons/bn.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/bn/lexicons/bn.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/romanized/\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/gu/\n",
            "dakshina_dataset_v1.0/gu/lexicons/\n",
            "dakshina_dataset_v1.0/gu/lexicons/gu.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/gu/lexicons/gu.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/gu/lexicons/gu.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/romanized/\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/hi/\n",
            "dakshina_dataset_v1.0/hi/lexicons/\n",
            "dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/hi/romanized/\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/kn/\n",
            "dakshina_dataset_v1.0/kn/lexicons/\n",
            "dakshina_dataset_v1.0/kn/lexicons/kn.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/kn/lexicons/kn.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/kn/lexicons/kn.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/kn/romanized/\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/ml/\n",
            "dakshina_dataset_v1.0/ml/lexicons/\n",
            "dakshina_dataset_v1.0/ml/lexicons/ml.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/ml/lexicons/tmp.rom.txt\n",
            "dakshina_dataset_v1.0/ml/lexicons/tmp.tsv\n",
            "dakshina_dataset_v1.0/ml/lexicons/ml.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/ml/lexicons/ml.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/romanized/\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/mr/\n",
            "dakshina_dataset_v1.0/mr/lexicons/\n",
            "dakshina_dataset_v1.0/mr/lexicons/mr.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/mr/lexicons/mr.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/mr/lexicons/mr.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/romanized/\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/pa/\n",
            "dakshina_dataset_v1.0/pa/lexicons/\n",
            "dakshina_dataset_v1.0/pa/lexicons/pa.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/pa/lexicons/pa.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/pa/lexicons/pa.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/pa/romanized/\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/sd/\n",
            "dakshina_dataset_v1.0/sd/lexicons/\n",
            "dakshina_dataset_v1.0/sd/lexicons/sd.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/sd/lexicons/sd.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/sd/lexicons/sd.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/sd/romanized/\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/si/\n",
            "dakshina_dataset_v1.0/si/lexicons/\n",
            "dakshina_dataset_v1.0/si/lexicons/si.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/si/lexicons/si.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/si/lexicons/si.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/romanized/\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/ta/\n",
            "dakshina_dataset_v1.0/ta/lexicons/\n",
            "dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/romanized/\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/te/\n",
            "dakshina_dataset_v1.0/te/lexicons/\n",
            "dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/romanized/\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/ur/\n",
            "dakshina_dataset_v1.0/ur/lexicons/\n",
            "dakshina_dataset_v1.0/ur/lexicons/ur.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/ur/lexicons/ur.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/ur/lexicons/ur.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/romanized/\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/README.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4OFs4DvTTtPB"
      },
      "outputs": [],
      "source": [
        "train_1 = load_data(\"/content/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.train.tsv\")\n",
        "dev_1 = load_data(\"/content/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.dev.tsv\")\n",
        "test_1 = load_data(\"/content/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.test.tsv\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_1 = train_1[train_1['hi'].notna()]\n",
        "train_1= train_1[train_1['en'].notna()]\n",
        "dev_1 = dev_1[dev_1['hi'].notna()]\n",
        "dev_1= dev_1[dev_1['en'].notna()]\n",
        "test_1 = test_1[test_1['hi'].notna()]\n",
        "test_1= test_1[test_1['en'].notna()]"
      ],
      "metadata": {
        "id": "LFEWdOnw6LSQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "0-JF8OnET2FL",
        "outputId": "b9fb55b0-49a5-4e45-f66a-d702f21dd226"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        hi        en\n",
              "0       अं        an\n",
              "1  अंकगणित  ankganit\n",
              "2     अंकल     uncle\n",
              "3    अंकुर     ankur\n",
              "4   अंकुरण   ankuran"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-99286d74-c028-473f-af25-5e9006ae3f14\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>hi</th>\n",
              "      <th>en</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>अं</td>\n",
              "      <td>an</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>अंकगणित</td>\n",
              "      <td>ankganit</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>अंकल</td>\n",
              "      <td>uncle</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>अंकुर</td>\n",
              "      <td>ankur</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>अंकुरण</td>\n",
              "      <td>ankuran</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-99286d74-c028-473f-af25-5e9006ae3f14')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-99286d74-c028-473f-af25-5e9006ae3f14 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-99286d74-c028-473f-af25-5e9006ae3f14');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 110
        }
      ],
      "source": [
        "train_1.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dqIMqB6cT2xE"
      },
      "outputs": [],
      "source": [
        "y = train_1['hi'].values\n",
        "x = train_1['en'].values\n",
        "y = '\\t'+y+'\\n'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xBLBQZGAebLH",
        "outputId": "3631bf21-141c-4078-e46d-ab6c15a30215"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['an', 'ankganit', 'uncle', ..., 'hyensang', 'xuanzang', 'om'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "srvhVGkAbctp",
        "outputId": "5901c940-fb16-400a-e452-2ec1fc8bd07e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['\\tअं\\n', '\\tअंकगणित\\n', '\\tअंकल\\n', ..., '\\tह्वेनसांग\\n',\n",
              "       '\\tह्वेनसांग\\n', '\\tॐ\\n'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xxl=[xx for xx,yy in zip(x,y)]\n",
        "english_char = set()\n",
        "hindi_char = set()\n",
        "for i in range(0,len(xxl)):\n",
        "  w1=xxl[i]\n",
        "  #print(w1)\n",
        "  for l in str(w1):\n",
        "    english_char.add(l)\n",
        "print(english_char)             #the set of all the letters\n",
        "\n",
        "yyl=[yy for xx,yy in zip(x,y)]\n",
        "\n",
        "for i in range(0,len(yyl)):\n",
        "  w2=yyl[i]\n",
        "  #print(w1)\n",
        "  for l in str(w2):\n",
        "    hindi_char.add(l)\n",
        "print(hindi_char)    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DyEFlIQFND_t",
        "outputId": "cbea2fbb-2306-49d8-a481-c3e42552f19f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'i', 'r', 'y', 'o', 'v', 'p', 's', 'n', 'b', 'k', 't', 'h', 'j', 'a', 'q', 'd', 'z', 'e', 'f', 'l', 'g', 'x', 'w', 'm', 'c', 'u'}\n",
            "{'अ', 'प', 'ौ', 'ठ', 'ण', 'य', 'ल', '\\t', 'ु', '्', 'ऊ', 'थ', 'ओ', 'ॉ', 'झ', 'ऋ', 'त', 'छ', 'र', 'े', 'ू', 'उ', 'ॐ', 'ब', 'ए', '\\n', 'ि', 'इ', 'ृ', 'ध', 'म', 'श', 'ं', 'ड', 'ख', 'क', 'ै', 'ह', 'ः', 'द', 'ज', 'ा', 'न', 'ग', 'ढ', 'ँ', 'ी', 'ट', 'ई', 'व', 'ञ', 'आ', '़', 'च', 'ङ', 'औ', 'ष', 'ॅ', 'भ', 'घ', 'फ', 'ऐ', 'ऑ', 'स', 'ो'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bEnBqQAqT5h-"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "english_char"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pzfQKTO8Zps_",
        "outputId": "29a650aa-dff1-4e5e-a330-0ad503dd7c5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'a',\n",
              " 'b',\n",
              " 'c',\n",
              " 'd',\n",
              " 'e',\n",
              " 'f',\n",
              " 'g',\n",
              " 'h',\n",
              " 'i',\n",
              " 'j',\n",
              " 'k',\n",
              " 'l',\n",
              " 'm',\n",
              " 'n',\n",
              " 'o',\n",
              " 'p',\n",
              " 'q',\n",
              " 'r',\n",
              " 's',\n",
              " 't',\n",
              " 'u',\n",
              " 'v',\n",
              " 'w',\n",
              " 'x',\n",
              " 'y',\n",
              " 'z'}"
            ]
          },
          "metadata": {},
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hindi_char"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iulI7BqiZvtK",
        "outputId": "c8cae2c9-80c7-4630-9753-88655cfe5dc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'\\t',\n",
              " '\\n',\n",
              " 'ँ',\n",
              " 'ं',\n",
              " 'ः',\n",
              " 'अ',\n",
              " 'आ',\n",
              " 'इ',\n",
              " 'ई',\n",
              " 'उ',\n",
              " 'ऊ',\n",
              " 'ऋ',\n",
              " 'ए',\n",
              " 'ऐ',\n",
              " 'ऑ',\n",
              " 'ओ',\n",
              " 'औ',\n",
              " 'क',\n",
              " 'ख',\n",
              " 'ग',\n",
              " 'घ',\n",
              " 'ङ',\n",
              " 'च',\n",
              " 'छ',\n",
              " 'ज',\n",
              " 'झ',\n",
              " 'ञ',\n",
              " 'ट',\n",
              " 'ठ',\n",
              " 'ड',\n",
              " 'ढ',\n",
              " 'ण',\n",
              " 'त',\n",
              " 'थ',\n",
              " 'द',\n",
              " 'ध',\n",
              " 'न',\n",
              " 'प',\n",
              " 'फ',\n",
              " 'ब',\n",
              " 'भ',\n",
              " 'म',\n",
              " 'य',\n",
              " 'र',\n",
              " 'ल',\n",
              " 'व',\n",
              " 'श',\n",
              " 'ष',\n",
              " 'स',\n",
              " 'ह',\n",
              " '़',\n",
              " 'ा',\n",
              " 'ि',\n",
              " 'ी',\n",
              " 'ु',\n",
              " 'ू',\n",
              " 'ृ',\n",
              " 'ॅ',\n",
              " 'े',\n",
              " 'ै',\n",
              " 'ॉ',\n",
              " 'ो',\n",
              " 'ौ',\n",
              " '्',\n",
              " 'ॐ'}"
            ]
          },
          "metadata": {},
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dct_eng= [(ch,i+1) for i,ch in enumerate(english_char)]\n",
        "english_char_map = dict(dct_eng)\n",
        "\n",
        "dct_hin= [(ch,i+1) for i,ch in enumerate(hindi_char)]\n",
        "hindi_char_map = dict(dct_hin)"
      ],
      "metadata": {
        "id": "ZTb8nnC4_Trj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "english_char_map"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7WjXtOocvRrG",
        "outputId": "fc5f76d2-3206-4895-b6c9-d43d7c6011c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'a': 14,\n",
              " 'b': 9,\n",
              " 'c': 25,\n",
              " 'd': 16,\n",
              " 'e': 18,\n",
              " 'f': 19,\n",
              " 'g': 21,\n",
              " 'h': 12,\n",
              " 'i': 1,\n",
              " 'j': 13,\n",
              " 'k': 10,\n",
              " 'l': 20,\n",
              " 'm': 24,\n",
              " 'n': 8,\n",
              " 'o': 4,\n",
              " 'p': 6,\n",
              " 'q': 15,\n",
              " 'r': 2,\n",
              " 's': 7,\n",
              " 't': 11,\n",
              " 'u': 26,\n",
              " 'v': 5,\n",
              " 'w': 23,\n",
              " 'x': 22,\n",
              " 'y': 3,\n",
              " 'z': 17}"
            ]
          },
          "metadata": {},
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0RPNSUvJbr_p"
      },
      "outputs": [],
      "source": [
        "num=0\n",
        "hindi_char_map[\" \"] = num\n",
        "english_char_map[\" \"] = num"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "english_char_map"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q0ep-LOZv9YK",
        "outputId": "b2775ef9-e12e-4789-e7af-7fa63c48bcbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{' ': 0,\n",
              " 'a': 14,\n",
              " 'b': 9,\n",
              " 'c': 25,\n",
              " 'd': 16,\n",
              " 'e': 18,\n",
              " 'f': 19,\n",
              " 'g': 21,\n",
              " 'h': 12,\n",
              " 'i': 1,\n",
              " 'j': 13,\n",
              " 'k': 10,\n",
              " 'l': 20,\n",
              " 'm': 24,\n",
              " 'n': 8,\n",
              " 'o': 4,\n",
              " 'p': 6,\n",
              " 'q': 15,\n",
              " 'r': 2,\n",
              " 's': 7,\n",
              " 't': 11,\n",
              " 'u': 26,\n",
              " 'v': 5,\n",
              " 'w': 23,\n",
              " 'x': 22,\n",
              " 'y': 3,\n",
              " 'z': 17}"
            ]
          },
          "metadata": {},
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len_m_eng= [len(i) for i in x]\n",
        "max_eng_len = max(len_m_eng)\n",
        "len_m_hin= [len(i) for i in y]\n",
        "max_hin_len = max(len_m_hin)"
      ],
      "metadata": {
        "id": "nMOS4Zfk_2L3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_eng_len"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JC0qvCmZxjZG",
        "outputId": "dc2e4900-732c-4d1c-ba4c-446d7cdd4a5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "metadata": {},
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c_5jSPrBbr_q"
      },
      "outputs": [],
      "source": [
        "def process(data):\n",
        "    x,y = data['en'].values, data['hi'].values\n",
        "    y = \"\\t\" + y + \"\\n\"\n",
        "    \n",
        "    a = np.zeros((len(x),max_eng_len),dtype=\"float32\")\n",
        "    b = np.zeros((len(y),max_hin_len),dtype=\"float32\")\n",
        "    c = np.zeros((len(y),max_hin_len,len(hindi_char)+1),dtype=\"int\")\n",
        "    \n",
        "    \n",
        "    for i,(xx,yy) in enumerate(zip(x,y)):\n",
        "        for j,ch in enumerate(xx):\n",
        "            a[i,j] = english_char_map[ch]\n",
        "\n",
        "        a[i,j+1:] = english_char_map[\" \"]\n",
        "        for j,ch in enumerate(yy):\n",
        "            b[i,j] = hindi_char_map[ch]\n",
        "\n",
        "            if j>0:\n",
        "                c[i,j-1,hindi_char_map[ch]] = 1\n",
        "\n",
        "        b[i,j+1:] = hindi_char_map[\" \"]\n",
        "        c[i,j:,hindi_char_map[\" \"]] = 1\n",
        "        \n",
        "    return a,b,c"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a8FMzKqvT9eA"
      },
      "outputs": [],
      "source": [
        "trainx, trainxx, trainy = process(train_1)\n",
        "valx, valxx, valy = process(dev_1)\n",
        "testx,testxx,testy = process(test_1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "testx"
      ],
      "metadata": {
        "id": "OHAjxb0y1p8O",
        "outputId": "e74ba09b-b258-438a-b455-3dc4d4611c46",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[14.,  8., 10., ...,  0.,  0.,  0.],\n",
              "       [14.,  8., 10., ...,  0.,  0.,  0.],\n",
              "       [14.,  8., 10., ...,  0.,  0.,  0.],\n",
              "       ...,\n",
              "       [12.,  4.,  7., ...,  0.,  0.,  0.],\n",
              "       [12.,  4.,  7., ...,  0.,  0.,  0.],\n",
              "       [12.,  4.,  7., ...,  0.,  0.,  0.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VNWQZNUCUHtL"
      },
      "outputs": [],
      "source": [
        "np.random.seed(25)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rev_dst_eng= [(i,char) for char,i in english_char_map.items()]\n",
        "reverse_eng_map = dict(rev_dst_eng)\n",
        "rev_dst_hin= [(i,char) for char,i in hindi_char_map.items()]\n",
        "reverse_hin_map = dict(rev_dst_hin)"
      ],
      "metadata": {
        "id": "viriP8OaBqXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reverse_eng_map"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AIOKGjMs-ygu",
        "outputId": "fd7f47ac-dfb9-4bce-ab9e-bc6f5ec3ad87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: ' ',\n",
              " 1: 'i',\n",
              " 2: 'r',\n",
              " 3: 'y',\n",
              " 4: 'o',\n",
              " 5: 'v',\n",
              " 6: 'p',\n",
              " 7: 's',\n",
              " 8: 'n',\n",
              " 9: 'b',\n",
              " 10: 'k',\n",
              " 11: 't',\n",
              " 12: 'h',\n",
              " 13: 'j',\n",
              " 14: 'a',\n",
              " 15: 'q',\n",
              " 16: 'd',\n",
              " 17: 'z',\n",
              " 18: 'e',\n",
              " 19: 'f',\n",
              " 20: 'l',\n",
              " 21: 'g',\n",
              " 22: 'x',\n",
              " 23: 'w',\n",
              " 24: 'm',\n",
              " 25: 'c',\n",
              " 26: 'u'}"
            ]
          },
          "metadata": {},
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J55EnhClbr_s"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0QD4O8xqbr_t"
      },
      "outputs": [],
      "source": [
        "#physical_devices = tf.config.list_physical_devices('GPU')\n",
        "#tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import SimpleRNN,LSTM,GRU,Embedding,Dense,Dropout,Input,Concatenate\n",
        "from tensorflow.keras.optimizers import Adam,Nadam\n",
        "from keras import Model"
      ],
      "metadata": {
        "id": "ZJejHIMBCOEy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Attention(tf.keras.layers.Layer):\n",
        "    def __init__(self, units,name=None, **kwargs):\n",
        "        super(Attention, self).__init__(name=name)\n",
        "        self.W1 = Dense(units,name='w1')\n",
        "        self.W2 = Dense(units,name='w2')\n",
        "        self.V = Dense(1,name='v')\n",
        "        self.units = units\n",
        "        super(Attention, self).__init__(**kwargs)\n",
        "\n",
        "    def call(self, tup):\n",
        "        # query hidden state shape == (batch_size, hidden size)\n",
        "        # query_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "        # values shape == (batch_size, max_len, hidden size)\n",
        "        # we are doing this to broadcast addition along the time axis to calculate the score\n",
        "        query = tup[0]\n",
        "        values = tup[1]\n",
        "        query_with_time_axis = tf.expand_dims(query, 1)\n",
        "        # score shape == (batch_size, max_length, 1)\n",
        "        # we get 1 at the last axis because we are applying score to self.V\n",
        "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
        "        score = self.V(tf.nn.tanh(\n",
        "            self.W1(query_with_time_axis) + self.W2(values)))\n",
        "\n",
        "        # attention_weights shape == (batch_size, max_length, 1)\n",
        "        attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "        # context_vector shape after sum == (batch_size, hidden_size)\n",
        "        context_vector = attention_weights * values\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "        \n",
        "\n",
        "        return context_vector, attention_weights\n",
        "    \n",
        "        \n",
        "    def get_config(self):\n",
        "        config = super(Attention,self).get_config()\n",
        "        config.update({\n",
        "            'units': self.units\n",
        "        })\n",
        "        return config"
      ],
      "metadata": {
        "id": "sSH6wlHzSmFO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "y-TLFDX8V_N6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dqIeHFkabzcU"
      },
      "outputs": [],
      "source": [
        "def lstm_model(cell = \"LSTM\",nunits = 32, enc_layers = 1, dec_layers = 1,embed_dim = 32,dense_size=32,dropout=None):\n",
        "    keras.backend.clear_session()\n",
        "    encoder_inputs = Input(shape=(None,),name='input1')\n",
        "    encoder_embedding = Embedding(input_dim=len(english_char)+1,output_dim = embed_dim,mask_zero=True,name=\"enc_embed\")\n",
        "    encoder_context = encoder_embedding(encoder_inputs)\n",
        "    decoder_inputs = Input(shape=(None,),name='input2')\n",
        "    decoder_embedding = Embedding(input_dim = len(hindi_char)+1,output_dim = embed_dim,mask_zero=True,name=\"dec_embed\")\n",
        "    decoder_context = decoder_embedding(decoder_inputs)\n",
        "    attention = Attention(nunits,name='attention')\n",
        "    tot_out = []\n",
        "    concat1 = Concatenate(axis=-1,name='concat1')\n",
        "    concat2 = Concatenate(axis=1,name='concat2')    \n",
        "    if cell == \"LSTM\":\n",
        "        encoder_prev = [LSTM(nunits,return_sequences=True,name=f\"enc_{i}\") for i in range(enc_layers-1)]\n",
        "        encoder_fin = LSTM(nunits,return_sequences=True,return_state=True,name=f\"enc_{enc_layers-1}\")\n",
        "        temp = encoder_context\n",
        "        for x,lay in enumerate(encoder_prev):\n",
        "            temp = lay(temp)\n",
        "            if dropout is not None:\n",
        "                temp = Dropout(dropout,name=f'do_{x}')(temp)\n",
        "            \n",
        "        enc_out = encoder_fin(temp)\n",
        "        dec_states = enc_out[1:]\n",
        "        \n",
        "        decoder = [LSTM(nunits,return_sequences=True,return_state=True,name=f\"dec_{i}\") for i in range(dec_layers)]\n",
        "        temp_states = [dec_states]*dec_layers\n",
        "        \n",
        "        for i in range(max_hin_len):\n",
        "            tup = (temp_states[0][0],enc_out[0])\n",
        "            context,att_wts = attention(tup)\n",
        "            temp = concat1([tf.expand_dims(context, 1), decoder_context[:,i:i+1,:]])\n",
        "#             temp = tf.expand_dims(temp, 1)\n",
        "            for i in range(dec_layers):\n",
        "                temp,sh,sc = decoder[i](temp,initial_state=temp_states[i])\n",
        "                temp_states[i] = [sh,sc]\n",
        "            tot_out.append(temp)\n",
        "            \n",
        "        outt = concat2(tot_out)\n",
        "       \n",
        "\n",
        "            \n",
        "        \n",
        "    dense_lay1 = Dense(dense_size,activation='relu',name='dense1')\n",
        "    pre_out = dense_lay1(outt)\n",
        "    dense_lay2 = Dense(len(hindi_char)+1,activation = 'softmax',name='dense2')\n",
        "    final_output = dense_lay2(pre_out)\n",
        "    \n",
        "    train = Model([encoder_inputs,decoder_inputs],final_output)\n",
        "    \n",
        "    return train\n",
        "#     return train"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def GRU_model(cell = \"GRU\",nunits = 32, enc_layers = 1, dec_layers = 1,embed_dim = 32,dense_size=32,dropout=None):\n",
        "    keras.backend.clear_session()\n",
        "    encoder_inputs = Input(shape=(None,),name='input1')\n",
        "    encoder_embedding = Embedding(input_dim=len(english_char)+1,output_dim = embed_dim,mask_zero=True,name=\"enc_embed\")\n",
        "    encoder_context = encoder_embedding(encoder_inputs)\n",
        "    decoder_inputs = Input(shape=(None,),name='input2')\n",
        "    decoder_embedding = Embedding(input_dim = len(hindi_char)+1,output_dim = embed_dim,mask_zero=True,name=\"dec_embed\")\n",
        "    decoder_context = decoder_embedding(decoder_inputs)\n",
        "    attention = Attention(nunits,name='attention')\n",
        "    tot_out = []\n",
        "    concat1 = Concatenate(axis=-1,name='concat1')\n",
        "    concat2 = Concatenate(axis=1,name='concat2')    \n",
        "    if cell == \"GRU\":\n",
        "        encoder_prev = [GRU(nunits,return_sequences=True,name=f\"enc_{i}\") for i in range(enc_layers-1)]\n",
        "        encoder_fin = GRU(nunits,return_sequences=True,return_state=True,name=f\"enc_{enc_layers-1}\")\n",
        "        temp = encoder_context\n",
        "        for x,lay in enumerate(encoder_prev):\n",
        "            temp = lay(temp)\n",
        "            if dropout is not None:\n",
        "                temp = Dropout(dropout,name=f'do_{x}')(temp)\n",
        "            \n",
        "        enc_out = encoder_fin(temp)\n",
        "        dec_states = enc_out[1:]\n",
        "        \n",
        "        decoder = [GRU(nunits,return_sequences=True,return_state=True,name=f\"dec_{i}\") for i in range(dec_layers)]\n",
        "        temp_states = []\n",
        "        for _ in range(dec_layers):\n",
        "            temp_states += dec_states\n",
        "        \n",
        "        for i in range(max_hin_len):\n",
        "            tup = (temp_states[0],enc_out[0])\n",
        "            context,att_wts = attention(tup)\n",
        "            temp = concat1([tf.expand_dims(context, 1), decoder_context[:,i:i+1,:]])\n",
        "#             temp = tf.expand_dims(temp, 1)\n",
        "            for i in range(dec_layers):\n",
        "                temp,st = decoder[i](temp,initial_state=temp_states[i])\n",
        "                temp_states[i] = st\n",
        "            tot_out.append(temp)\n",
        "            \n",
        "        outt = concat2(tot_out)\n",
        "       \n",
        "\n",
        "            \n",
        "        \n",
        "    dense_lay1 = Dense(dense_size,activation='relu',name='dense1')\n",
        "    pre_out = dense_lay1(outt)\n",
        "    dense_lay2 = Dense(len(hindi_char)+1,activation = 'softmax',name='dense2')\n",
        "    final_output = dense_lay2(pre_out)\n",
        "    \n",
        "    train = Model([encoder_inputs,decoder_inputs],final_output)\n",
        "    \n",
        "    return train"
      ],
      "metadata": {
        "id": "58z3RCFbWf_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#for wandb\n",
        "sweep_config = {\n",
        "    'method': 'bayes', #grid, random, bayes\n",
        "    'metric': {\n",
        "      'name': 'val_accuracy',\n",
        "      'goal': 'maximize'   \n",
        "    },\n",
        "    'parameters': {\n",
        "        'cell': {\n",
        "            'values': [\"LSTM\",\"GRU\"]\n",
        "        },\n",
        "        'embed_size': {\n",
        "            'values': [32,128,256]\n",
        "        },\n",
        "        'lr': {\n",
        "            'values': [1e-2,1e-3]\n",
        "        },\n",
        "        'dense_size': {\n",
        "            'values': [64,128,512]\n",
        "        },\n",
        "        'dropout': {\n",
        "            'values': [0.0,0.2,0.5]\n",
        "        },\n",
        "        'nunits': {\n",
        "            'values': [128,256]\n",
        "        },\n",
        "        'enc_layers': {\n",
        "            'values': [1,3,5]\n",
        "        },\n",
        "        'dec_layers': {\n",
        "            'values': [1,3,5]\n",
        "        },\n",
        "        \n",
        "    }\n",
        "}\n"
      ],
      "metadata": {
        "id": "3aNH3oLOQpZ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sweep_id = wandb.sweep(sweep_config,project=\"a=3\", entity=\"dl22\")"
      ],
      "metadata": {
        "id": "WBEY-K2KSabu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cnWOlB5Bv5O3"
      },
      "outputs": [],
      "source": [
        "def train():\n",
        "    # Default values for hyper-parameters we're going to sweep over\n",
        "    config_defaults = {\n",
        "        'lr': 1e-2,\n",
        "        'dense_size': 64,\n",
        "        'nunits': 128,\n",
        "        'cell': 'LSTM',\n",
        "        'embed_size': 64,\n",
        "        'enc_layers': 1,\n",
        "        'dec_layers': 1,\n",
        "        'dropout': 0.\n",
        "    }\n",
        "\n",
        "    # Initialize a new wandb run\n",
        "    wandb.init(config=config_defaults,name=\"cs6910\")\n",
        "    \n",
        "    cfg = wandb.config\n",
        "    \n",
        "    name = f'nunits_{cfg.nunits}_cell_{cfg.cell}_encl_{cfg.enc_layers}_decl_{cfg.dec_layers}_emb_{cfg.embed_size}_ds_{cfg.dense_size}_do_{cfg.dropout}_lr_{cfg.lr}'\n",
        "    wandb.run.name = name\n",
        "    wandb.run.save()\n",
        "    \n",
        "    # Config is a variable that holds and saves hyperparameters and inputs\n",
        "    \n",
        "    train,enc,dec = lstm_model(nunits=cfg.nunits,\n",
        "                                dense_size=cfg.dense_size,\n",
        "                                enc_layers=cfg.enc_layers,\n",
        "                                dec_layers=cfg.dec_layers,\n",
        "                                cell = cfg.cell,\n",
        "                                dropout = cfg.dropout,\n",
        "                                embed_dim = cfg.embed_size)\n",
        "    train.compile(optimizer = Adam(lr=cfg.lr),loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "    print(\"model building done\")\n",
        "    train.fit([trainx,trainxx],trainy,\n",
        "             batch_size=64,\n",
        "             validation_data = ([valx,valxx],valy),\n",
        "             epochs=10,\n",
        "             callbacks = [WandbCallback(monitor='val_accuracy',mode='min')])\n",
        "    print(\"model training done\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#wandb.agent(sweep_id, train,count=25)"
      ],
      "metadata": {
        "id": "k7SCbcjqVC7J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nhD81K6Gbr_v"
      },
      "outputs": [],
      "source": [
        "def accuracy1(real,pred):\n",
        "    real = tf.math.argmax(real,axis=2)\n",
        "    pred = tf.math.argmax(pred,axis=2)\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    acc = tf.math.equal(real,pred)\n",
        "    mask = tf.cast(mask, dtype='int32')\n",
        "    acc = tf.cast(acc, dtype='int32')\n",
        "    acc = tf.math.multiply(acc,mask)\n",
        "    mask = tf.reduce_sum(mask,axis=1)\n",
        "    acc = tf.reduce_sum(acc,axis=1)\n",
        "    acc = tf.math.equal(acc,mask)\n",
        "    acc = tf.cast(acc, dtype='float32')\n",
        "    return tf.reduce_mean(acc)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train = lstm_model(nunits=256,\n",
        "                                dense_size=512,\n",
        "                                enc_layers=3,\n",
        "                                dec_layers=1,\n",
        "                                cell = \"LSTM\",\n",
        "                                dropout = 0.2,\n",
        "                                embed_dim = 256)\n",
        "train.compile(optimizer = Adam(lr=1e-3),loss='categorical_crossentropy',metrics=[accuracy1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1zNKuToBCczq",
        "outputId": "8e95f43c-1f01-4364-fb2a-917dcbcff972"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZF1kzHAYbr_w"
      },
      "outputs": [],
      "source": [
        "model_cb = tf.keras.callbacks.ModelCheckpoint('best_model.h5',monitor='val_accuracy1',mode='max',save_best_only=True,verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "xEbM_QEhbr_w",
        "outputId": "518fd131-a939-414f-912e-a581586214b2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "691/691 [==============================] - ETA: 0s - loss: 0.8907 - accuracy1: 0.0016\n",
            "Epoch 1: val_accuracy1 improved from -inf to 0.01404, saving model to best_model.h5\n",
            "691/691 [==============================] - 674s 889ms/step - loss: 0.8907 - accuracy1: 0.0016 - val_loss: 0.5166 - val_accuracy1: 0.0140\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f1bfd5617d0>"
            ]
          },
          "metadata": {},
          "execution_count": 142
        }
      ],
      "source": [
        "train.fit([trainx,trainxx],trainy,\n",
        "             batch_size=64,\n",
        "             validation_data = ([valx,valxx],valy),\n",
        "             epochs=1,\n",
        "             callbacks = [model_cb])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y_Pkt-9Wbr_x"
      },
      "outputs": [],
      "source": [
        "train.save('best_model_attention.h5')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "SDhvNQWDbPS_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2SzABHEKbc7V"
      },
      "outputs": [],
      "source": [
        "def GRU_inference(model,nunits=32,enc_layers=1,dec_layers=1,cell='LSTM',dropout=None):\n",
        "    keras.backend.clear_session()\n",
        "    encoder_inputs = model.input[0]\n",
        "    encoder_embedding = model.get_layer('enc_embed')\n",
        "    encoder_context = encoder_embedding(encoder_inputs)\n",
        "    decoder_inputs = model.input[1]\n",
        "    decoder_embedding = model.get_layer('dec_embed')\n",
        "    decoder_context = decoder_embedding(decoder_inputs)\n",
        "    \n",
        "    \n",
        "    encoder_prev = [model.get_layer(f'enc_{i}') for i in range(enc_layers-1)]\n",
        "    encoder_fin = model.get_layer(f'enc_{enc_layers-1}')\n",
        "    temp = encoder_context\n",
        "    for i,lay in enumerate(encoder_prev):\n",
        "        temp = lay(temp)\n",
        "        if dropout is not None:\n",
        "            temp = model.get_layer(f'do_{i}')(temp)\n",
        "     \n",
        "        \n",
        "    if cell == \"GRU\":\n",
        "        enc_out, state = encoder_fin(temp)\n",
        "        enc_final = [enc_out,state]\n",
        "\n",
        "    encoder_model = keras.models.Model(encoder_inputs,enc_final)\n",
        "    \n",
        "    \n",
        "    decoder = [model.get_layer(f'dec_{i}') for i in range(dec_layers)]\n",
        "    \n",
        "    attention = model.get_layer('attention')\n",
        "    \n",
        "    concat1 = model.get_layer('concat1')\n",
        "\n",
        "\n",
        "    if cell == \"GRU\":\n",
        "        state_inputs = []\n",
        "        state_outputs = []\n",
        "        \n",
        "        encout_input = Input(shape=(None,nunits),name='inputenc')\n",
        "        \n",
        "        temp = decoder_context[:,:1,:]\n",
        "                                                                  \n",
        "        for i in range(dec_layers):\n",
        "            state_input = Input(shape=(nunits,),name=f\"inputs{i}\")\n",
        "            \n",
        "            if i==0:\n",
        "                tup = (state_input,encout_input)\n",
        "                context,att_wts_out = attention(tup)\n",
        "                temp = concat1([tf.expand_dims(context, 1), temp])\n",
        "                \n",
        "            temp,s = decoder[i](temp,initial_state = state_input)\n",
        "            state_inputs.append(state_input)\n",
        "            state_outputs.append(s)\n",
        "            \n",
        "        decoder_input_pass = [decoder_inputs,encout_input] + state_inputs\n",
        "\n",
        "    pre_out = model.get_layer('dense1')(temp)\n",
        "    final_output = model.get_layer('dense2')(pre_out)\n",
        "\n",
        "    decoder_model = keras.models.Model(decoder_input_pass, [final_output,att_wts_out]+state_outputs)\n",
        "    \n",
        "    return encoder_model,decoder_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PQF8g1N3bzcZ"
      },
      "outputs": [],
      "source": [
        "def lstm_inference(model,nunits=32,enc_layers=1,dec_layers=1,cell='LSTM',dropout=None):\n",
        "    keras.backend.clear_session()\n",
        "    encoder_inputs = model.input[0]\n",
        "    encoder_embedding = model.get_layer('enc_embed')\n",
        "    encoder_context = encoder_embedding(encoder_inputs)\n",
        "    decoder_inputs = model.input[1]\n",
        "    decoder_embedding = model.get_layer('dec_embed')\n",
        "    decoder_context = decoder_embedding(decoder_inputs)\n",
        "    \n",
        "    \n",
        "    encoder_prev = [model.get_layer(f'enc_{i}') for i in range(enc_layers-1)]\n",
        "    encoder_fin = model.get_layer(f'enc_{enc_layers-1}')\n",
        "    temp = encoder_context\n",
        "    for i,lay in enumerate(encoder_prev):\n",
        "        temp = lay(temp)\n",
        "        if dropout is not None:\n",
        "            temp = model.get_layer(f'do_{i}')(temp)\n",
        "     \n",
        "    if cell == \"LSTM\":\n",
        "        enc_out, state_h,state_c = encoder_fin(temp)\n",
        "        enc_final = [enc_out,state_h,state_c]\n",
        "        \n",
        "\n",
        "    encoder_model = keras.models.Model(encoder_inputs,enc_final)\n",
        "    \n",
        "    \n",
        "    decoder = [model.get_layer(f'dec_{i}') for i in range(dec_layers)]\n",
        "    \n",
        "    attention = model.get_layer('attention')\n",
        "    \n",
        "    concat1 = model.get_layer('concat1')\n",
        "\n",
        "    if cell == \"LSTM\":\n",
        "        state_inputs = []\n",
        "        state_outputs = []\n",
        "        \n",
        "        encout_input = Input(shape=(None,nunits),name='inputenc')\n",
        "        \n",
        "        temp = decoder_context[:,-1:,:]\n",
        "                                                                  \n",
        "        for i in range(dec_layers):\n",
        "            decoder_input_h = Input(shape=(nunits,),name=f\"inputh{i}\")\n",
        "            decoder_input_c = Input(shape=(nunits,),name=f\"inputc{i}\")\n",
        "            \n",
        "            if i==0:\n",
        "                tup = (decoder_input_h,encout_input)\n",
        "                context,att_wts_out = attention(tup)\n",
        "                temp = concat1([tf.expand_dims(context, 1), temp])\n",
        "                \n",
        "            temp,sh,sc = decoder[i](temp,initial_state = [decoder_input_h,decoder_input_c])\n",
        "            state_inputs += [decoder_input_h,decoder_input_c]\n",
        "            state_outputs += [sh,sc]\n",
        "            \n",
        "        decoder_input_pass = [decoder_inputs,encout_input] + state_inputs\n",
        "\n",
        "    pre_out = model.get_layer('dense1')(temp)\n",
        "    final_output = model.get_layer('dense2')(pre_out)\n",
        "\n",
        "    decoder_model = keras.models.Model(decoder_input_pass, [final_output,att_wts_out]+state_outputs)\n",
        "    \n",
        "    return encoder_model,decoder_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P62ezOf5br_y"
      },
      "outputs": [],
      "source": [
        "enc,dec = lstm_inference(train,nunits=256,enc_layers=3,dec_layers=1,cell=\"LSTM\",dropout='yes')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HgTUhJKebr_z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75df06a0-8d61-4f50-d89f-16c991364fce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ],
      "source": [
        "enc.save('best_enc.h5')\n",
        "dec.save('best_dec.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eptVMhEtbzcc"
      },
      "outputs": [],
      "source": [
        "def beam_search(inp,k,dec_layers,cell=\"LSTM\"):\n",
        "    enc_out = enc.predict(inp)\n",
        "    statess = enc_out[1:]\n",
        "    target_seq = np.zeros((inp.shape[0],1))\n",
        "    target_seq[:,0] = hindi_char_map[\"\\t\"]\n",
        "    if cell == \"LSTM\":\n",
        "        states = []\n",
        "        for i in range(dec_layers):\n",
        "            states += [statess[0],statess[1]]\n",
        "    else:\n",
        "        states = []\n",
        "        for i in range(dec_layers):\n",
        "            states += [statess]\n",
        "            \n",
        "    output = dec.predict([target_seq,enc_out[0]]+states)\n",
        "    states = output[2:]\n",
        "    \n",
        "    stat1 = np.asarray(states).transpose([1,0,2])\n",
        "    \n",
        "    best_chars = np.argsort(output[0][:,-1,:],axis=-1)[:,-k:]\n",
        "    scores = np.sort(output[0][:,-1,:],axis=-1)[:,-k:]\n",
        "    sequences = [[([ch],-np.log(sc),stat1[i],0,output[1][i]) for ch,sc in zip(best_chars[i],scores[i])] for i in range(inp.shape[0])]\n",
        "    \n",
        "    for t1 in range(max_hin_len-1):\n",
        "        candidates = [[] for _ in range(inp.shape[0])]\n",
        "        for j in range(k):\n",
        "            target_seq[:,0] = [sequences[i][j][0][-1] for i in range(inp.shape[0])]\n",
        "            states = list(np.asarray([sequences[i][j][2] for i in range(inp.shape[0])]).transpose([1,0,2]))\n",
        "            output = dec.predict([target_seq,enc_out[0]]+states,batch_size=32)\n",
        "            best_chars = np.argsort(output[0][:,-1,:],axis=-1)[:,-k:]\n",
        "            scores = np.sort(output[0][:,-1,:],axis=-1)[:,-k:]\n",
        "            \n",
        "            stat1 = np.asarray(output[2:]).transpose([1,0,2])\n",
        "            \n",
        "            for i in range(inp.shape[0]):\n",
        "                chk = 1 if (sequences[i][j][3]==1 or sequences[i][j][0][-1] == hindi_char_map[\"\\n\"]) else 0\n",
        "                if chk == 0:\n",
        "                    candidates[i] += [(sequences[i][j][0]+[best_chars[i,rep]],\n",
        "                                       sequences[i][j][1]-np.log(scores[i,rep]),\n",
        "                                       stat1[i],\n",
        "                                       chk,\n",
        "                                       np.concatenate((sequences[i][j][4],output[1][i]),axis=1))\n",
        "                                      for rep in range(k)]\n",
        "                else:\n",
        "                    candidates[i] += [sequences[i][j]]\n",
        "                    \n",
        "        for i in range(inp.shape[0]):\n",
        "            candidates[i] = sorted(candidates[i],key = lambda tup:tup[1]/len(tup[0]))\n",
        "            sequences[i] = candidates[i][:k]\n",
        "            \n",
        "        print(f\"decoder {t1} done\")\n",
        "            \n",
        "        \n",
        "    res = [list() for i in range(inp.shape[0])]\n",
        "    att_wts = [list() for i in range(inp.shape[0])]\n",
        "    for i in range(inp.shape[0]):\n",
        "        for j in range(k):\n",
        "            res[i].append(sequences[i][j][0])\n",
        "            att_wts[i].append(sequences[i][j][4])\n",
        "        \n",
        "    return res,att_wts"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "start = time.time()\n",
        "pred1,att_wts = beam_search(testx,5,1,cell=\"GRU\")\n",
        "print(time.time()-start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQaCSVNPccoM",
        "outputId": "79c28132-ade4-4a29-bcd0-0059734b1cfb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "decoder 0 done\n",
            "decoder 1 done\n",
            "decoder 2 done\n",
            "decoder 3 done\n",
            "decoder 4 done\n",
            "decoder 5 done\n",
            "decoder 6 done\n",
            "decoder 7 done\n",
            "decoder 8 done\n",
            "decoder 9 done\n",
            "decoder 10 done\n",
            "decoder 11 done\n",
            "decoder 12 done\n",
            "decoder 13 done\n",
            "decoder 14 done\n",
            "decoder 15 done\n",
            "decoder 16 done\n",
            "decoder 17 done\n",
            "decoder 18 done\n",
            "decoder 19 done\n",
            "177.1892054080963\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ans = []\n",
        "ans1 = []\n",
        "acc = 0\n",
        "\n",
        "for i,pre in enumerate(pred1):\n",
        "    word = []\n",
        "    word1 = []\n",
        "    \n",
        "    orig = \"\"\n",
        "    for ch in testx[i]:\n",
        "        if reverse_eng_map[ch] == \" \":\n",
        "            break\n",
        "        orig += reverse_eng_map[ch]\n",
        "    word.append(orig)\n",
        "    word1.append(orig)\n",
        "    \n",
        "    hind = \"\"\n",
        "    for ch in testxx[i,1:]:\n",
        "        if reverse_hin_map[ch] == \"\\n\":\n",
        "            break\n",
        "        hind += reverse_hin_map[ch]\n",
        "    \n",
        "    word.append(hind)\n",
        "    word1.append(hind)\n",
        "    \n",
        "    fl=0\n",
        "    \n",
        "    for j,pr in enumerate(pre):\n",
        "        deco1 = \"\"\n",
        "        for ch in pr:\n",
        "            if reverse_hin_map[ch] == \"\\n\":\n",
        "                break\n",
        "            deco1 += reverse_hin_map[ch]\n",
        "        word.append(deco1)\n",
        "        if j==0:\n",
        "            word1.append(deco1)\n",
        "            \n",
        "        \n",
        "            if hind==deco1:\n",
        "                fl=1\n",
        "            \n",
        "    if fl==1:\n",
        "        acc += 1\n",
        "        \n",
        "    ans.append(word)\n",
        "    ans1.append(word1)\n",
        "    \n",
        "print(acc/len(ans))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJrlGN_UdGo9",
        "outputId": "a6d243cb-17a0-4cad-e4b2-4439260db4c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.02265659706796979\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(ans,columns=['English','Hindi']+[f'Hindi_pred_{i}' for i in range(5)])\n",
        "df.sample(n=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "id": "-1Gr2EJDciXd",
        "outputId": "bd5f4ca0-693c-41d1-ffac-ba41cde73826"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          English      Hindi Hindi_pred_0 Hindi_pred_1 Hindi_pred_2  \\\n",
              "1787    darmiyaan    दरमियान   दर्यामियों      दर्भानी      दर्धानी   \n",
              "2257       purple      पर्पल      स्रिक्र      स्रेट्ट      स्रेट्र   \n",
              "2663      bandhan      बंधान         बंगल         बंधन        बंधान   \n",
              "3086  maryaadaaon  मर्यादाओं     मारायकान      माराधान     मारहाकान   \n",
              "1127         char        चार          चार           चर          दीर   \n",
              "\n",
              "     Hindi_pred_3 Hindi_pred_4  \n",
              "1787     दर्भानिय     दर्यामीय  \n",
              "2257      प्रेट्ट      स्रेक्र  \n",
              "2663         बंदल        बंदला  \n",
              "3086     मारायकां    मारायाकान  \n",
              "1127          फाल          चाल  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-71b36158-5d9e-423e-ae59-1086fd9678d1\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>English</th>\n",
              "      <th>Hindi</th>\n",
              "      <th>Hindi_pred_0</th>\n",
              "      <th>Hindi_pred_1</th>\n",
              "      <th>Hindi_pred_2</th>\n",
              "      <th>Hindi_pred_3</th>\n",
              "      <th>Hindi_pred_4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1787</th>\n",
              "      <td>darmiyaan</td>\n",
              "      <td>दरमियान</td>\n",
              "      <td>दर्यामियों</td>\n",
              "      <td>दर्भानी</td>\n",
              "      <td>दर्धानी</td>\n",
              "      <td>दर्भानिय</td>\n",
              "      <td>दर्यामीय</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2257</th>\n",
              "      <td>purple</td>\n",
              "      <td>पर्पल</td>\n",
              "      <td>स्रिक्र</td>\n",
              "      <td>स्रेट्ट</td>\n",
              "      <td>स्रेट्र</td>\n",
              "      <td>प्रेट्ट</td>\n",
              "      <td>स्रेक्र</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2663</th>\n",
              "      <td>bandhan</td>\n",
              "      <td>बंधान</td>\n",
              "      <td>बंगल</td>\n",
              "      <td>बंधन</td>\n",
              "      <td>बंधान</td>\n",
              "      <td>बंदल</td>\n",
              "      <td>बंदला</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3086</th>\n",
              "      <td>maryaadaaon</td>\n",
              "      <td>मर्यादाओं</td>\n",
              "      <td>मारायकान</td>\n",
              "      <td>माराधान</td>\n",
              "      <td>मारहाकान</td>\n",
              "      <td>मारायकां</td>\n",
              "      <td>मारायाकान</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1127</th>\n",
              "      <td>char</td>\n",
              "      <td>चार</td>\n",
              "      <td>चार</td>\n",
              "      <td>चर</td>\n",
              "      <td>दीर</td>\n",
              "      <td>फाल</td>\n",
              "      <td>चाल</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-71b36158-5d9e-423e-ae59-1086fd9678d1')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-71b36158-5d9e-423e-ae59-1086fd9678d1 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-71b36158-5d9e-423e-ae59-1086fd9678d1');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 153
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('predictions_attention_beam.csv')"
      ],
      "metadata": {
        "id": "lEO5iBhJdCCc"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "attention_final.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}