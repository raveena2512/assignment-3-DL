{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "reference:https://keras.io/examples/nlp/lstm_seq2seq/"
      ],
      "metadata": {
        "id": "1MYk0_izwf69"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "reference: https://machinelearningmastery.com/define-encoder-decoder-sequence-sequence-model-neural-machine-translation-keras/"
      ],
      "metadata": {
        "id": "G1U2YR4jwaPT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6VgLpCUITeG0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb\n",
        "import wandb\n",
        "from wandb.keras import WandbCallback"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aqLE759CP8QC",
        "outputId": "026cc71a-60fb-40ae-ab61-2c71ec402361"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.7/dist-packages (0.12.16)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.5.12)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.1.27)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.7/dist-packages (from wandb) (0.1.2)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from wandb) (57.4.0)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.0.9)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.2)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.7/dist-packages (from wandb) (1.2.3)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.0.9)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.2.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (5.0.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8RMV62EDbr_l"
      },
      "outputs": [],
      "source": [
        "def load_data(path):\n",
        "    with open(path) as fil:\n",
        "        data = pd.read_csv(fil,sep='\\t',header=None,names=[\"hi\",\"en\",\"\"],skip_blank_lines=True,index_col=None)\n",
        "    data = data[['hi','en']]\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the dataset\n",
        "!curl https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar --output daksh.tar\n",
        "# Extract the downloaded tar file\n",
        "!tar -xvf  'daksh.tar'\n",
        "# Set the file paths to train, validation and test dataset\n",
        "train_file=os.path.join(os.getcwd(),\"dakshina_dataset_v1.0\",\"hi\",\"lexicons\",\"hi.translit.sampled.train.tsv\")\n",
        "vaildation_file_path = os.path.join(os.getcwd(),\"dakshina_dataset_v1.0\",\"hi\",\"lexicons\",\"hi.translit.sampled.dev.tsv\")\n",
        "test_file_path = os.path.join(os.getcwd(),\"dakshina_dataset_v1.0\",\"hi\",\"lexicons\",\"hi.translit.sampled.test.tsv\")"
      ],
      "metadata": {
        "id": "jQd76Coie5TM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0f46bbc-1485-475e-e5d6-89a197b49050"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 1915M  100 1915M    0     0   219M      0  0:00:08  0:00:08 --:--:--  203M\n",
            "dakshina_dataset_v1.0/bn/\n",
            "dakshina_dataset_v1.0/bn/lexicons/\n",
            "dakshina_dataset_v1.0/bn/lexicons/bn.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/bn/lexicons/bn.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/bn/lexicons/bn.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/romanized/\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/gu/\n",
            "dakshina_dataset_v1.0/gu/lexicons/\n",
            "dakshina_dataset_v1.0/gu/lexicons/gu.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/gu/lexicons/gu.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/gu/lexicons/gu.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/romanized/\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/hi/\n",
            "dakshina_dataset_v1.0/hi/lexicons/\n",
            "dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/hi/romanized/\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/kn/\n",
            "dakshina_dataset_v1.0/kn/lexicons/\n",
            "dakshina_dataset_v1.0/kn/lexicons/kn.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/kn/lexicons/kn.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/kn/lexicons/kn.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/kn/romanized/\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/ml/\n",
            "dakshina_dataset_v1.0/ml/lexicons/\n",
            "dakshina_dataset_v1.0/ml/lexicons/ml.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/ml/lexicons/tmp.rom.txt\n",
            "dakshina_dataset_v1.0/ml/lexicons/tmp.tsv\n",
            "dakshina_dataset_v1.0/ml/lexicons/ml.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/ml/lexicons/ml.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/romanized/\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/mr/\n",
            "dakshina_dataset_v1.0/mr/lexicons/\n",
            "dakshina_dataset_v1.0/mr/lexicons/mr.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/mr/lexicons/mr.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/mr/lexicons/mr.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/romanized/\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/pa/\n",
            "dakshina_dataset_v1.0/pa/lexicons/\n",
            "dakshina_dataset_v1.0/pa/lexicons/pa.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/pa/lexicons/pa.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/pa/lexicons/pa.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/pa/romanized/\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/sd/\n",
            "dakshina_dataset_v1.0/sd/lexicons/\n",
            "dakshina_dataset_v1.0/sd/lexicons/sd.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/sd/lexicons/sd.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/sd/lexicons/sd.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/sd/romanized/\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/si/\n",
            "dakshina_dataset_v1.0/si/lexicons/\n",
            "dakshina_dataset_v1.0/si/lexicons/si.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/si/lexicons/si.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/si/lexicons/si.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/romanized/\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/ta/\n",
            "dakshina_dataset_v1.0/ta/lexicons/\n",
            "dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/romanized/\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/te/\n",
            "dakshina_dataset_v1.0/te/lexicons/\n",
            "dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/romanized/\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/ur/\n",
            "dakshina_dataset_v1.0/ur/lexicons/\n",
            "dakshina_dataset_v1.0/ur/lexicons/ur.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/ur/lexicons/ur.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/ur/lexicons/ur.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/romanized/\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/README.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4OFs4DvTTtPB"
      },
      "outputs": [],
      "source": [
        "train_1 = load_data(\"/content/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.train.tsv\")\n",
        "dev_1 = load_data(\"/content/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.dev.tsv\")\n",
        "test_1 = load_data(\"/content/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.test.tsv\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_1 = train_1[train_1['hi'].notna()]\n",
        "train_1= train_1[train_1['en'].notna()]\n",
        "dev_1 = dev_1[dev_1['hi'].notna()]\n",
        "dev_1= dev_1[dev_1['en'].notna()]\n",
        "test_1 = test_1[test_1['hi'].notna()]\n",
        "test_1= test_1[test_1['en'].notna()]"
      ],
      "metadata": {
        "id": "LFEWdOnw6LSQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "0-JF8OnET2FL",
        "outputId": "1dac64ec-a87c-439b-a28a-9f785e276d43"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        hi        en\n",
              "0       अं        an\n",
              "1  अंकगणित  ankganit\n",
              "2     अंकल     uncle\n",
              "3    अंकुर     ankur\n",
              "4   अंकुरण   ankuran"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b393a796-610e-4e2e-8baa-000d0ef0c6c2\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>hi</th>\n",
              "      <th>en</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>अं</td>\n",
              "      <td>an</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>अंकगणित</td>\n",
              "      <td>ankganit</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>अंकल</td>\n",
              "      <td>uncle</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>अंकुर</td>\n",
              "      <td>ankur</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>अंकुरण</td>\n",
              "      <td>ankuran</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b393a796-610e-4e2e-8baa-000d0ef0c6c2')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b393a796-610e-4e2e-8baa-000d0ef0c6c2 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b393a796-610e-4e2e-8baa-000d0ef0c6c2');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 132
        }
      ],
      "source": [
        "train_1.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dqIMqB6cT2xE"
      },
      "outputs": [],
      "source": [
        "y = train_1['hi'].values\n",
        "x = train_1['en'].values\n",
        "y = '\\t'+y+'\\n'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xBLBQZGAebLH",
        "outputId": "87830739-128f-4f12-9fb9-bb36544cf02d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['an', 'ankganit', 'uncle', ..., 'hyensang', 'xuanzang', 'om'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 134
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "srvhVGkAbctp",
        "outputId": "1440b817-d5b2-49a0-a891-552c5a230510"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['\\tअं\\n', '\\tअंकगणित\\n', '\\tअंकल\\n', ..., '\\tह्वेनसांग\\n',\n",
              "       '\\tह्वेनसांग\\n', '\\tॐ\\n'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xxl=[xx for xx,yy in zip(x,y)]\n",
        "english_char = set()\n",
        "hindi_char = set()\n",
        "for i in range(0,len(xxl)):\n",
        "  w1=xxl[i]\n",
        "  #print(w1)\n",
        "  for l in str(w1):\n",
        "    english_char.add(l)\n",
        "print(english_char)             #the set of all the letters\n",
        "\n",
        "yyl=[yy for xx,yy in zip(x,y)]\n",
        "\n",
        "for i in range(0,len(yyl)):\n",
        "  w2=yyl[i]\n",
        "  #print(w1)\n",
        "  for l in str(w2):\n",
        "    hindi_char.add(l)\n",
        "print(hindi_char)    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DyEFlIQFND_t",
        "outputId": "b44962e8-f24f-46e3-e966-d5f75caee4a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'t', 'h', 'f', 'n', 'x', 'u', 's', 'd', 'z', 'o', 'v', 'c', 'y', 'b', 'l', 'a', 'i', 'm', 'q', 'e', 'k', 'w', 'g', 'p', 'r', 'j'}\n",
            "{'द', 'घ', 'प', 'ओ', 'ख', 'क', 'ग', 'ो', 'ॉ', 'ऊ', 'न', 'भ', '\\t', 'ँ', 'ल', 'ण', 'ह', 'आ', 'ा', 'ङ', 'ज', 'स', 'ष', 'त', 'झ', 'ऑ', 'फ', 'ं', 'च', '़', 'ठ', 'श', '्', 'ड', 'ौ', 'इ', 'ॅ', 'अ', 'ट', 'ए', '\\n', 'छ', 'म', 'उ', 'ध', 'ई', 'व', 'ी', 'ब', 'ृ', 'र', 'ढ', 'ः', 'ऋ', 'थ', 'े', 'ऐ', 'ि', 'य', 'औ', 'ू', 'ॐ', 'ञ', 'ु', 'ै'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bEnBqQAqT5h-"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "english_char"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pzfQKTO8Zps_",
        "outputId": "433434a8-68fc-4056-fea5-bcbfd48d8879"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'a',\n",
              " 'b',\n",
              " 'c',\n",
              " 'd',\n",
              " 'e',\n",
              " 'f',\n",
              " 'g',\n",
              " 'h',\n",
              " 'i',\n",
              " 'j',\n",
              " 'k',\n",
              " 'l',\n",
              " 'm',\n",
              " 'n',\n",
              " 'o',\n",
              " 'p',\n",
              " 'q',\n",
              " 'r',\n",
              " 's',\n",
              " 't',\n",
              " 'u',\n",
              " 'v',\n",
              " 'w',\n",
              " 'x',\n",
              " 'y',\n",
              " 'z'}"
            ]
          },
          "metadata": {},
          "execution_count": 137
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hindi_char"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iulI7BqiZvtK",
        "outputId": "ce531b1a-0947-44aa-e3b9-4e61919b4e85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'\\t',\n",
              " '\\n',\n",
              " 'ँ',\n",
              " 'ं',\n",
              " 'ः',\n",
              " 'अ',\n",
              " 'आ',\n",
              " 'इ',\n",
              " 'ई',\n",
              " 'उ',\n",
              " 'ऊ',\n",
              " 'ऋ',\n",
              " 'ए',\n",
              " 'ऐ',\n",
              " 'ऑ',\n",
              " 'ओ',\n",
              " 'औ',\n",
              " 'क',\n",
              " 'ख',\n",
              " 'ग',\n",
              " 'घ',\n",
              " 'ङ',\n",
              " 'च',\n",
              " 'छ',\n",
              " 'ज',\n",
              " 'झ',\n",
              " 'ञ',\n",
              " 'ट',\n",
              " 'ठ',\n",
              " 'ड',\n",
              " 'ढ',\n",
              " 'ण',\n",
              " 'त',\n",
              " 'थ',\n",
              " 'द',\n",
              " 'ध',\n",
              " 'न',\n",
              " 'प',\n",
              " 'फ',\n",
              " 'ब',\n",
              " 'भ',\n",
              " 'म',\n",
              " 'य',\n",
              " 'र',\n",
              " 'ल',\n",
              " 'व',\n",
              " 'श',\n",
              " 'ष',\n",
              " 'स',\n",
              " 'ह',\n",
              " '़',\n",
              " 'ा',\n",
              " 'ि',\n",
              " 'ी',\n",
              " 'ु',\n",
              " 'ू',\n",
              " 'ृ',\n",
              " 'ॅ',\n",
              " 'े',\n",
              " 'ै',\n",
              " 'ॉ',\n",
              " 'ो',\n",
              " 'ौ',\n",
              " '्',\n",
              " 'ॐ'}"
            ]
          },
          "metadata": {},
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dct_eng= [(ch,i+1) for i,ch in enumerate(english_char)]\n",
        "english_char_map = dict(dct_eng)\n",
        "\n",
        "dct_hin= [(ch,i+1) for i,ch in enumerate(hindi_char)]\n",
        "hindi_char_map = dict(dct_hin)"
      ],
      "metadata": {
        "id": "ZTb8nnC4_Trj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "english_char_map"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7WjXtOocvRrG",
        "outputId": "d8b4fa4b-78ce-400f-be25-40043ba4b7a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'a': 16,\n",
              " 'b': 14,\n",
              " 'c': 12,\n",
              " 'd': 8,\n",
              " 'e': 20,\n",
              " 'f': 3,\n",
              " 'g': 23,\n",
              " 'h': 2,\n",
              " 'i': 17,\n",
              " 'j': 26,\n",
              " 'k': 21,\n",
              " 'l': 15,\n",
              " 'm': 18,\n",
              " 'n': 4,\n",
              " 'o': 10,\n",
              " 'p': 24,\n",
              " 'q': 19,\n",
              " 'r': 25,\n",
              " 's': 7,\n",
              " 't': 1,\n",
              " 'u': 6,\n",
              " 'v': 11,\n",
              " 'w': 22,\n",
              " 'x': 5,\n",
              " 'y': 13,\n",
              " 'z': 9}"
            ]
          },
          "metadata": {},
          "execution_count": 140
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0RPNSUvJbr_p"
      },
      "outputs": [],
      "source": [
        "num=0\n",
        "hindi_char_map[\" \"] = num\n",
        "english_char_map[\" \"] = num"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "english_char_map"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q0ep-LOZv9YK",
        "outputId": "ed4b0460-725b-416b-d0ad-73f4387ada0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{' ': 0,\n",
              " 'a': 16,\n",
              " 'b': 14,\n",
              " 'c': 12,\n",
              " 'd': 8,\n",
              " 'e': 20,\n",
              " 'f': 3,\n",
              " 'g': 23,\n",
              " 'h': 2,\n",
              " 'i': 17,\n",
              " 'j': 26,\n",
              " 'k': 21,\n",
              " 'l': 15,\n",
              " 'm': 18,\n",
              " 'n': 4,\n",
              " 'o': 10,\n",
              " 'p': 24,\n",
              " 'q': 19,\n",
              " 'r': 25,\n",
              " 's': 7,\n",
              " 't': 1,\n",
              " 'u': 6,\n",
              " 'v': 11,\n",
              " 'w': 22,\n",
              " 'x': 5,\n",
              " 'y': 13,\n",
              " 'z': 9}"
            ]
          },
          "metadata": {},
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len_m_eng= [len(i) for i in x]\n",
        "max_eng_len = max(len_m_eng)\n",
        "len_m_hin= [len(i) for i in y]\n",
        "max_hin_len = max(len_m_hin)"
      ],
      "metadata": {
        "id": "nMOS4Zfk_2L3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_eng_len"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JC0qvCmZxjZG",
        "outputId": "4b0ac97c-2ecf-4d77-9895-6e0faaa6d30a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "metadata": {},
          "execution_count": 144
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c_5jSPrBbr_q"
      },
      "outputs": [],
      "source": [
        "def process(data):\n",
        "    x,y = data['en'].values, data['hi'].values\n",
        "    y = \"\\t\" + y + \"\\n\"\n",
        "    \n",
        "    a = np.zeros((len(x),max_eng_len),dtype=\"float32\")\n",
        "    b = np.zeros((len(y),max_hin_len),dtype=\"float32\")\n",
        "    c = np.zeros((len(y),max_hin_len,len(hindi_char)+1),dtype=\"int\")\n",
        "    \n",
        "    \n",
        "    for i,(xx,yy) in enumerate(zip(x,y)):\n",
        "        for j,ch in enumerate(xx):\n",
        "            a[i,j] = english_char_map[ch]\n",
        "\n",
        "        a[i,j+1:] = english_char_map[\" \"]\n",
        "        for j,ch in enumerate(yy):\n",
        "            b[i,j] = hindi_char_map[ch]\n",
        "\n",
        "            if j>0:\n",
        "                c[i,j-1,hindi_char_map[ch]] = 1\n",
        "\n",
        "        b[i,j+1:] = hindi_char_map[\" \"]\n",
        "        c[i,j:,hindi_char_map[\" \"]] = 1\n",
        "        \n",
        "    return a,b,c"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a8FMzKqvT9eA"
      },
      "outputs": [],
      "source": [
        "trainx, trainxx, trainy = process(train_1)\n",
        "valx, valxx, valy = process(dev_1)\n",
        "testx,testxx,testy = process(test_1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "testx"
      ],
      "metadata": {
        "id": "OHAjxb0y1p8O",
        "outputId": "1144dbd0-23cd-42a0-d726-4da9628bb10d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[16.,  4., 21., ...,  0.,  0.,  0.],\n",
              "       [16.,  4., 21., ...,  0.,  0.,  0.],\n",
              "       [16.,  4., 21., ...,  0.,  0.,  0.],\n",
              "       ...,\n",
              "       [ 2., 10.,  7., ...,  0.,  0.,  0.],\n",
              "       [ 2., 10.,  7., ...,  0.,  0.,  0.],\n",
              "       [ 2., 10.,  7., ...,  0.,  0.,  0.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 147
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VNWQZNUCUHtL"
      },
      "outputs": [],
      "source": [
        "np.random.seed(25)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rev_dst_eng= [(i,char) for char,i in english_char_map.items()]\n",
        "reverse_eng_map = dict(rev_dst_eng)\n",
        "rev_dst_hin= [(i,char) for char,i in hindi_char_map.items()]\n",
        "reverse_hin_map = dict(rev_dst_hin)"
      ],
      "metadata": {
        "id": "viriP8OaBqXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reverse_eng_map"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AIOKGjMs-ygu",
        "outputId": "43064396-bc55-4e21-86f4-88d3c6e68424"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: ' ',\n",
              " 1: 't',\n",
              " 2: 'h',\n",
              " 3: 'f',\n",
              " 4: 'n',\n",
              " 5: 'x',\n",
              " 6: 'u',\n",
              " 7: 's',\n",
              " 8: 'd',\n",
              " 9: 'z',\n",
              " 10: 'o',\n",
              " 11: 'v',\n",
              " 12: 'c',\n",
              " 13: 'y',\n",
              " 14: 'b',\n",
              " 15: 'l',\n",
              " 16: 'a',\n",
              " 17: 'i',\n",
              " 18: 'm',\n",
              " 19: 'q',\n",
              " 20: 'e',\n",
              " 21: 'k',\n",
              " 22: 'w',\n",
              " 23: 'g',\n",
              " 24: 'p',\n",
              " 25: 'r',\n",
              " 26: 'j'}"
            ]
          },
          "metadata": {},
          "execution_count": 150
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J55EnhClbr_s"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0QD4O8xqbr_t"
      },
      "outputs": [],
      "source": [
        "physical_devices = tf.config.list_physical_devices('GPU')\n",
        "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import SimpleRNN,LSTM,GRU,Embedding,Dense,Dropout,Input\n",
        "from tensorflow.keras.optimizers import Adam,Nadam\n",
        "from keras import Model"
      ],
      "metadata": {
        "id": "ZJejHIMBCOEy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def GRU_model(cell = \"GRU\",nunits = 32, enc_layers = 1, dec_layers = 1,embed_dim = 32,dense_size=32,dropout=None):\n",
        "    keras.backend.clear_session()\n",
        "    encoder_inputs = Input(shape=(None,))\n",
        "    encoder_embedding = Embedding(input_dim=len(english_char)+1,output_dim = embed_dim,mask_zero=True,name=\"enc_embed\")\n",
        "    encoder_context = encoder_embedding(encoder_inputs)\n",
        "    decoder_inputs = Input(shape=(None,))\n",
        "    decoder_embedding = Embedding(input_dim = len(hindi_char)+1,output_dim = embed_dim,mask_zero=True,name=\"dec_embed\")\n",
        "    decoder_context = decoder_embedding(decoder_inputs)\n",
        "\n",
        "    if cell == \"GRU\":\n",
        "        encoder_prev = [GRU(nunits,return_sequences=True,name=f\"enc_{i}\") for i in range(enc_layers-1)]\n",
        "        encoder_fin = GRU(nunits,return_state=True,name=f\"enc_{enc_layers-1}\")\n",
        "        temp = encoder_context\n",
        "        for i,lay in enumerate(encoder_prev):\n",
        "            temp = lay(temp)\n",
        "            if dropout is not None:\n",
        "                temp = Dropout(dropout,name=f\"do_{i}\")(temp)\n",
        "            \n",
        "        _, state = encoder_fin(temp)\n",
        "        encoder_states = state\n",
        "        \n",
        "        decoder = [GRU(nunits,return_sequences=True,return_state=True,name=f\"dec_{i}\") for i in range(dec_layers)]\n",
        "        \n",
        "        temp,s = decoder[0](decoder_context,initial_state=state)\n",
        "        for i in range(1,dec_layers):\n",
        "            temp,s = decoder[i](temp,initial_state=state)\n",
        "            \n",
        "        \n",
        "    dense_lay1 = Dense(dense_size,activation='relu',name='dense1')\n",
        "    pre_out = dense_lay1(temp)\n",
        "    dense_lay2 = Dense(len(hindi_char)+1,activation = 'softmax',name='dense2')\n",
        "    final_output = dense_lay2(pre_out)\n",
        "    \n",
        "    train = Model([encoder_inputs,decoder_inputs],final_output)\n",
        "    \n",
        "    return train\n"
      ],
      "metadata": {
        "id": "1w8At1Ag4tx-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def SimpleRNN(cell = \"Simple\",nunits = 32, enc_layers = 1, dec_layers = 1,embed_dim = 32,dense_size=32,dropout=None):\n",
        "    keras.backend.clear_session()\n",
        "    encoder_inputs = Input(shape=(None,))\n",
        "    encoder_embedding = Embedding(input_dim=len(english_char)+1,output_dim = embed_dim,mask_zero=True,name=\"enc_embed\")\n",
        "    encoder_context = encoder_embedding(encoder_inputs)\n",
        "    decoder_inputs = Input(shape=(None,))\n",
        "    decoder_embedding = Embedding(input_dim = len(hindi_char)+1,output_dim = embed_dim,mask_zero=True,name=\"dec_embed\")\n",
        "    decoder_context = decoder_embedding(decoder_inputs)\n",
        "\n",
        "    if cell == \"Simple\":\n",
        "        encoder_prev = [SimpleRNN(nunits,return_sequences=True,name=f\"enc_{i}\") for i in range(enc_layers-1)]\n",
        "        encoder_fin = SimpleRNN(nunits,return_state=True,name=f\"enc_{enc_layers-1}\")\n",
        "        temp = encoder_context\n",
        "        for i,lay in enumerate(encoder_prev):\n",
        "            temp = lay(temp)\n",
        "            if dropout is not None:\n",
        "                temp = Dropout(dropout,name=f\"do_{i}\")(temp)\n",
        "            \n",
        "        _, state = encoder_fin(temp)\n",
        "        encoder_states = state\n",
        "        \n",
        "        decoder = [SimpleRNN(nunits,return_sequences=True,return_state=True,name=f\"dec_{i}\") for i in range(dec_layers)]\n",
        "        \n",
        "        temp,s = decoder[0](decoder_context,initial_state=state)\n",
        "        for i in range(1,dec_layers):\n",
        "            temp,s = decoder[i](temp,initial_state=state)\n",
        "\n",
        "    dense_lay1 = Dense(dense_size,activation='relu',name='dense1')\n",
        "    pre_out = dense_lay1(temp)\n",
        "    dense_lay2 = Dense(len(hindi_char)+1,activation = 'softmax',name='dense2')\n",
        "    final_output = dense_lay2(pre_out)\n",
        "    \n",
        "    train = Model([encoder_inputs,decoder_inputs],final_output)\n",
        "    \n",
        "    return train"
      ],
      "metadata": {
        "id": "EHOsBDYLFD_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lstm_model(cell = \"LSTM\",nunits = 32, enc_layers = 1, dec_layers = 1,embed_dim = 32,dense_size=32,dropout=None):\n",
        "    keras.backend.clear_session()\n",
        "    encoder_inputs = Input(shape=(None,))\n",
        "    encoder_embedding = Embedding(input_dim=len(english_char)+1,output_dim = embed_dim,mask_zero=True,name=\"enc_embed\")\n",
        "    encoder_context = encoder_embedding(encoder_inputs)\n",
        "    decoder_inputs = Input(shape=(None,))\n",
        "    decoder_embedding = Embedding(input_dim = len(hindi_char)+1,output_dim = embed_dim,mask_zero=True,name=\"dec_embed\")\n",
        "    decoder_context = decoder_embedding(decoder_inputs)\n",
        "\n",
        "    if cell == \"LSTM\":\n",
        "        encoder_prev = [LSTM(nunits,return_sequences=True,name=f\"enc_{i}\") for i in range(enc_layers-1)]\n",
        "        encoder_fin = LSTM(nunits,return_state=True,name=f\"enc_{enc_layers-1}\")\n",
        "        temp = encoder_context\n",
        "        for i,lay in enumerate(encoder_prev):\n",
        "            temp = lay(temp)\n",
        "            if dropout is not None:\n",
        "                temp = Dropout(dropout,name=f\"do_{i}\")(temp)\n",
        "            \n",
        "        _, state_h,state_c = encoder_fin(temp)\n",
        "        encoder_states = [state_h,state_c]\n",
        "        \n",
        "        decoder = [LSTM(nunits,return_sequences=True,return_state=True,name=f\"dec_{i}\") for i in range(dec_layers)]\n",
        "        \n",
        "        temp,sh,sc = decoder[0](decoder_context,initial_state=encoder_states)\n",
        "        for i in range(1,dec_layers):\n",
        "            temp,sh,sc = decoder[i](temp,initial_state=encoder_states)\n",
        "            \n",
        "    \n",
        "    dense_lay1 = Dense(dense_size,activation='relu',name='dense1')\n",
        "    pre_out = dense_lay1(temp)\n",
        "    dense_lay2 = Dense(len(hindi_char)+1,activation = 'softmax',name='dense2')\n",
        "    final_output = dense_lay2(pre_out)\n",
        "    \n",
        "    train = Model([encoder_inputs,decoder_inputs],final_output)\n",
        "    \n",
        "    return train\n"
      ],
      "metadata": {
        "id": "rtsiywj3CFq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#for wandb\n",
        "sweep_config = {\n",
        "    'method': 'bayes', #grid, random, bayes\n",
        "    'metric': {\n",
        "      'name': 'val_accuracy',\n",
        "      'goal': 'maximize'   \n",
        "    },\n",
        "    'parameters': {\n",
        "        'cell': {\n",
        "            'values': [\"LSTM\",\"GRU\",\"Simple\"]\n",
        "        },\n",
        "        'embed_size': {\n",
        "            'values': [32,128,256]\n",
        "        },\n",
        "        'lr': {\n",
        "            'values': [1e-2,1e-3]\n",
        "        },\n",
        "        'dense_size': {\n",
        "            'values': [64,128,512]\n",
        "        },\n",
        "        'dropout': {\n",
        "            'values': [0.0,0.2,0.5]\n",
        "        },\n",
        "        'nunits': {\n",
        "            'values': [128,256]\n",
        "        },\n",
        "        'enc_layers': {\n",
        "            'values': [1,3,5]\n",
        "        },\n",
        "        'dec_layers': {\n",
        "            'values': [1,3,5]\n",
        "        },\n",
        "        \n",
        "    }\n",
        "}\n"
      ],
      "metadata": {
        "id": "3aNH3oLOQpZ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sweep_id = wandb.sweep(sweep_config,project=\"a=3\", entity=\"dl22\")"
      ],
      "metadata": {
        "id": "WBEY-K2KSabu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cnWOlB5Bv5O3"
      },
      "outputs": [],
      "source": [
        "def train():\n",
        "    # Default values for hyper-parameters we're going to sweep over\n",
        "    config_defaults = {\n",
        "        'lr': 1e-2,\n",
        "        'dense_size': 64,\n",
        "        'nunits': 128,\n",
        "        'cell': 'LSTM',\n",
        "        'embed_size': 64,\n",
        "        'enc_layers': 1,\n",
        "        'dec_layers': 1,\n",
        "        'dropout': 0.\n",
        "    }\n",
        "\n",
        "    # Initialize a new wandb run\n",
        "    wandb.init(config=config_defaults,name=\"cs6910\")\n",
        "    \n",
        "    cfg = wandb.config\n",
        "    \n",
        "    name = f'nunits_{cfg.nunits}_cell_{cfg.cell}_encl_{cfg.enc_layers}_decl_{cfg.dec_layers}_emb_{cfg.embed_size}_ds_{cfg.dense_size}_do_{cfg.dropout}_lr_{cfg.lr}'\n",
        "    wandb.run.name = name\n",
        "    wandb.run.save()\n",
        "    \n",
        "    # Config is a variable that holds and saves hyperparameters and inputs\n",
        "    \n",
        "    train,enc,dec = lstm_model(nunits=cfg.nunits,\n",
        "                                dense_size=cfg.dense_size,\n",
        "                                enc_layers=cfg.enc_layers,\n",
        "                                dec_layers=cfg.dec_layers,\n",
        "                                cell = cfg.cell,\n",
        "                                dropout = cfg.dropout,\n",
        "                                embed_dim = cfg.embed_size)\n",
        "    train.compile(optimizer = Adam(lr=cfg.lr),loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "    print(\"model building done\")\n",
        "    train.fit([trainx,trainxx],trainy,\n",
        "             batch_size=64,\n",
        "             validation_data = ([valx,valxx],valy),\n",
        "             epochs=10,\n",
        "             callbacks = [WandbCallback(monitor='val_accuracy',mode='min')])\n",
        "    print(\"model training done\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#wandb.agent(sweep_id, train,count=25)"
      ],
      "metadata": {
        "id": "k7SCbcjqVC7J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nhD81K6Gbr_v"
      },
      "outputs": [],
      "source": [
        "def accuracy1(real,pred):\n",
        "    real = tf.math.argmax(real,axis=2)\n",
        "    pred = tf.math.argmax(pred,axis=2)\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    acc = tf.math.equal(real,pred)\n",
        "    mask = tf.cast(mask, dtype='int32')\n",
        "    acc = tf.cast(acc, dtype='int32')\n",
        "    acc = tf.math.multiply(acc,mask)\n",
        "    mask = tf.reduce_sum(mask,axis=1)\n",
        "    acc = tf.reduce_sum(acc,axis=1)\n",
        "    acc = tf.math.equal(acc,mask)\n",
        "    acc = tf.cast(acc, dtype='float32')\n",
        "    return tf.reduce_mean(acc)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train = lstm_model(nunits=256,\n",
        "                                dense_size=512,\n",
        "                                enc_layers=3,\n",
        "                                dec_layers=1,\n",
        "                                cell = \"LSTM\",\n",
        "                                dropout = 0.2,\n",
        "                                embed_dim = 256)\n",
        "train.compile(optimizer = Adam(lr=1e-3),loss='categorical_crossentropy',metrics=[accuracy1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1zNKuToBCczq",
        "outputId": "e56f72cc-aaee-4076-faed-cb2542186a03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZF1kzHAYbr_w"
      },
      "outputs": [],
      "source": [
        "model_cb = tf.keras.callbacks.ModelCheckpoint('best_model.h5',monitor='val_accuracy1',mode='max',save_best_only=True,verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "xEbM_QEhbr_w",
        "outputId": "4b9e40ee-6754-4089-a488-3ff726c6b4b4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "691/691 [==============================] - ETA: 0s - loss: 0.7499 - accuracy1: 0.0122\n",
            "Epoch 1: val_accuracy1 improved from -inf to 0.07150, saving model to best_model.h5\n",
            "691/691 [==============================] - 49s 32ms/step - loss: 0.7499 - accuracy1: 0.0122 - val_loss: 0.3929 - val_accuracy1: 0.0715\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe3fe995790>"
            ]
          },
          "metadata": {},
          "execution_count": 164
        }
      ],
      "source": [
        "train.fit([trainx,trainxx],trainy,\n",
        "             batch_size=64,\n",
        "             validation_data = ([valx,valxx],valy),\n",
        "             epochs=1,\n",
        "             callbacks = [model_cb])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y_Pkt-9Wbr_x"
      },
      "outputs": [],
      "source": [
        "model = keras.models.load_model('best_model.h5',custom_objects={'accuracy1':accuracy1})"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def GRU_inference(model,nunits=32,enc_layers=1,dec_layers=1,cell='LSTM',dropout=None):\n",
        "    encoder_inputs = model.input[0]\n",
        "    encoder_embedding = model.get_layer('enc_embed')\n",
        "    encoder_context = encoder_embedding(encoder_inputs)\n",
        "    decoder_inputs = model.input[1]\n",
        "    decoder_embedding = model.get_layer('dec_embed')\n",
        "    decoder_context = decoder_embedding(decoder_inputs)\n",
        "    \n",
        "    \n",
        "    encoder_prev = [model.get_layer(f'enc_{i}') for i in range(enc_layers-1)]\n",
        "    encoder_fin = model.get_layer(f'enc_{enc_layers-1}')\n",
        "    temp = encoder_context\n",
        "    for i,lay in enumerate(encoder_prev):\n",
        "        temp = lay(temp)\n",
        "        if dropout is not None:\n",
        "            temp = model.get_layer(f'do_{i}')(temp)\n",
        "    decoder = [model.get_layer(f'dec_{i}') for i in range(dec_layers)]\n",
        "    if cell == \"GRU\":\n",
        "        state_inputs = []\n",
        "        state_outputs = []\n",
        "\n",
        "        state_input = Input(shape=(nunits,),name='inputs0')\n",
        "        temp,s = decoder[0](decoder_context,initial_state = state_input)\n",
        "        state_inputs.append(state_input)\n",
        "        state_outputs.append(s)\n",
        "\n",
        "        for i in range(1,dec_layers):\n",
        "            state_input = Input(shape=(nunits,),name=f'inputs{i}')\n",
        "            temp,s = decoder[i](temp,initial_state = state_input)\n",
        "            state_inputs.append(state_input)\n",
        "            state_outputs.append(s)\n",
        "\n",
        "        decoder_input_pass = [decoder_inputs] + state_inputs\n",
        "\n",
        "    pre_out = model.get_layer('dense1')(temp)\n",
        "    final_output = model.get_layer('dense2')(pre_out)\n",
        "\n",
        "    decoder_model = keras.models.Model(decoder_input_pass, [final_output]+state_outputs)\n",
        "    \n",
        "    return encoder_model,decoder_model"
      ],
      "metadata": {
        "id": "D7_eSazGNlor"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lstm_inference(model,nunits=32,enc_layers=1,dec_layers=1,cell='LSTM',dropout=None):\n",
        "    encoder_inputs = model.input[0]\n",
        "    encoder_embedding = model.get_layer('enc_embed')\n",
        "    encoder_context = encoder_embedding(encoder_inputs)\n",
        "    decoder_inputs = model.input[1]\n",
        "    decoder_embedding = model.get_layer('dec_embed')\n",
        "    decoder_context = decoder_embedding(decoder_inputs)\n",
        "    encoder_prev = [model.get_layer(f'enc_{i}') for i in range(enc_layers-1)]\n",
        "    encoder_fin = model.get_layer(f'enc_{enc_layers-1}')\n",
        "    temp = encoder_context\n",
        "    for i,lay in enumerate(encoder_prev):\n",
        "        temp = lay(temp)\n",
        "        if dropout is not None:\n",
        "            temp = model.get_layer(f'do_{i}')(temp)\n",
        "    if cell == \"LSTM\":\n",
        "        _, state_h,state_c = encoder_fin(temp)\n",
        "        encoder_states = [state_h,state_c]\n",
        "    encoder_model = keras.models.Model(encoder_inputs,encoder_states)\n",
        "    \n",
        "    \n",
        "    \n",
        "    decoder = [model.get_layer(f'dec_{i}') for i in range(dec_layers)]\n",
        "    if cell == \"LSTM\":\n",
        "        state_inputs = []\n",
        "        state_outputs = []\n",
        "\n",
        "        decoder_input_h = Input(shape=(nunits,),name='inputh0')\n",
        "        decoder_input_c = Input(shape=(nunits,),name='inputc0')\n",
        "        temp,sh,sc = decoder[0](decoder_context,initial_state = [decoder_input_h,decoder_input_c])\n",
        "        state_inputs += [decoder_input_h,decoder_input_c]\n",
        "        state_outputs += [sh,sc]\n",
        "\n",
        "        for i in range(1,dec_layers):\n",
        "            decoder_input_h = Input(shape=(nunits,),name=f'inputh{i}')\n",
        "            decoder_input_c = Input(shape=(nunits,),name=f'inputc{i}')\n",
        "            temp,sh,sc = decoder[i](temp,initial_state = [decoder_input_h,decoder_input_c])\n",
        "            state_inputs += [decoder_input_h,decoder_input_c]\n",
        "            state_outputs += [sh,sc]\n",
        "\n",
        "        decoder_input_pass = [decoder_inputs] + state_inputs\n",
        "\n",
        "    pre_out = model.get_layer('dense1')(temp)\n",
        "    final_output = model.get_layer('dense2')(pre_out)\n",
        "\n",
        "    decoder_model = keras.models.Model(decoder_input_pass, [final_output]+state_outputs)\n",
        "    \n",
        "    return encoder_model,decoder_model\n"
      ],
      "metadata": {
        "id": "wg0RpcbEL6t7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P62ezOf5br_y"
      },
      "outputs": [],
      "source": [
        "enc,dec = lstm_inference(model,nunits=256,enc_layers=3,dec_layers=1,cell=\"LSTM\",dropout='yes')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HgTUhJKebr_z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e37d273-7ba0-4199-9047-a4ab6dae36d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ],
      "source": [
        "enc.save('best_enc.h5')\n",
        "dec.save('best_dec.h5')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "final_a3-train.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}